
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DaRE Cross-domain recommender on Amazon Reviews dataset &#8212; coldstart-recsys</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="AGGN Cold-start Recommendation on ML-100k" href="T290734_AGGN_Cold_start_Recommendation_on_ML_100k.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">coldstart-recsys</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L281872_Cold_Start_Recommendations.html">
   Cold-Start Recommendations
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T847725_Attribute_to_Feature_Mappings_for_Cold_Start_Recommendations.html">
   Attribute to Feature Mappings for Cold-Start Recommendations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T457846_LightFM_Cold_start_on_ML_10m.html">
   LightFM Cold-start on ML-10m
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T459379_EMCDR_on_MovieLens_Netflix_dataset.html">
   EMCDR on MovieLens-Netflix dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T229879_HERS_Cold_start_Recommendations_on_LastFM_dataset.html">
   HERS Cold-start Recommendations on LastFM dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T316836_Collective_Matrix_Factorization_on_ML_1m.html">
   Collective Matrix Factorization on ML-1m
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T490340_Double_domain_recommendations_on_ML_1m.html">
   Double domain recommendations on ML-1m
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T951866_DropoutNet_Cold_start_Recommendation_on_CiteULike_dataset.html">
   DropoutNet Cold-start Recommendation on CiteULike dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T714933_MetaTL_for_Cold_start_users_on_Amazon_Electronics_dataset.html">
   MetaTL for Cold-start users on Amazon Electronics dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T722684_TaNP_Cold_start_Recommender_on_LastFM_dataset.html">
   TaNP Cold-start Recommender on LastFM dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T290734_AGGN_Cold_start_Recommendation_on_ML_100k.html">
   AGGN Cold-start Recommendation on ML-100k
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   DaRE Cross-domain recommender on Amazon Reviews dataset
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T519611_DaRE_Cross_domain_recommender_on_Amazon_Reviews_dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/coldstart-recsys/main?urlpath=tree/docs/T519611_DaRE_Cross_domain_recommender_on_Amazon_Reviews_dataset.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/coldstart-recsys/blob/main/docs/T519611_DaRE_Cross_domain_recommender_on_Amazon_Reviews_dataset.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-statement">
   Problem Statement
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-architecture">
   Model Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-procedure">
   Training Procedure
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#params">
     Params
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading">
     Loading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing">
     Preprocessing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-definition">
   Model Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clean-strings-for-reviews">
   Clean strings for reviews
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review-embedding-layer">
   Review embedding layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-function">
   Training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation-inference-function">
   Validation &amp; Inference function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extra-notes">
   Extra Notes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-process">
     Inference Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#domain-aware-feature-extraction-example">
     Domain-Aware Feature Extraction Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-encoder-example">
     Review Encoder Example
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="dare-cross-domain-recommender-on-amazon-reviews-dataset">
<h1>DaRE Cross-domain recommender on Amazon Reviews dataset<a class="headerlink" href="#dare-cross-domain-recommender-on-amazon-reviews-dataset" title="Permalink to this headline">Â¶</a></h1>
<p>CDR utilizes information from source domains to alleviate the cold-start problem in the target domain. Early studies adopt feature mapping technique that requires overlapped users. For example, RC-DFM applies Stacked Denoising Autoencoder (SDAE) to each domain, where the learned knowledge of the same set of users are transferred from source to target domain. To overcome the restrictive requirement of overlapped users, CDLFM and CATN employ neighbor or similar user-based feature mapping. However, this kind of cross-domain algorithm implicates defects like filtering noises or requiring duplicate users.</p>
<div class="section" id="problem-statement">
<h2>Problem Statement<a class="headerlink" href="#problem-statement" title="Permalink to this headline">Â¶</a></h2>
<p>Assume two datasets, <span class="math notranslate nohighlight">\(ğ·^ğ‘ \)</span> and <span class="math notranslate nohighlight">\(ğ·^ğ‘¡\)</span>, be the information from the source and target domains, respectively. Each dataset consists of tuples, <span class="math notranslate nohighlight">\((ğ‘¢,ğ‘–,ğ‘¦_{ğ‘¢,ğ‘–}, ğ‘Ÿ_{ğ‘¢,ğ‘–})\)</span> which represents an individual review <span class="math notranslate nohighlight">\(ğ‘Ÿ_{ğ‘¢,ğ‘–}\)</span> written by a user ğ‘¢ for item ğ‘– with a rating <span class="math notranslate nohighlight">\(ğ‘¦_{ğ‘¢,ğ‘–}\)</span>. The two datasets take the form of <span class="math notranslate nohighlight">\(D^s = (ğ‘¢^s,ğ‘–^s,ğ‘¦^s_{ğ‘¢,ğ‘–}, ğ‘Ÿ^s_{ğ‘¢,ğ‘–})\)</span> and <span class="math notranslate nohighlight">\(D^t = (ğ‘¢^t,ğ‘–^t,ğ‘¦^t_{ğ‘¢,ğ‘–}, ğ‘Ÿ^t_{ğ‘¢,ğ‘–})\)</span>, respectively. The goal of our task is to predict an accurate rating score <span class="math notranslate nohighlight">\(y^t_{u,i}\)</span> using <span class="math notranslate nohighlight">\(ğ·^ğ‘ \)</span> and a partial set of <span class="math notranslate nohighlight">\(ğ·^t\)</span>.</p>
</div>
<div class="section" id="model-architecture">
<h2>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">Â¶</a></h2>
<p><center><img src='_images/T519611_1.png'></center></p></div>
<div class="section" id="training-procedure">
<h2>Training Procedure<a class="headerlink" href="#training-procedure" title="Permalink to this headline">Â¶</a></h2>
<p>The training phase starts with review embedding layers followed by three types of feature extractors, <span class="math notranslate nohighlight">\({ğ¹ğ¸}^ğ‘ \)</span>, <span class="math notranslate nohighlight">\({ğ¹ğ¸}^c\)</span>, and <span class="math notranslate nohighlight">\({ğ¹ğ¸}^t\)</span>, named source, common, and target, for the separation of domain-specific, domain-common knowledge. Integrated with domain discriminator, three FEs are trained independently for the parallel extraction of domain-specific <span class="math notranslate nohighlight">\(ğ‘‚^ğ‘ \)</span>, <span class="math notranslate nohighlight">\(ğ‘‚^ğ‘¡\)</span> and domain-common knowledge <span class="math notranslate nohighlight">\(ğ‘‚^{ğ‘,ğ‘ }\)</span>, <span class="math notranslate nohighlight">\(ğ‘‚^{ğ‘,ğ‘¡}\)</span>.</p>
<p><center><img src='_images/T519611_2.png'></center></p><p>Then, for each domain, the review encoder generates a single vector <span class="math notranslate nohighlight">\(ğ¸^ğ‘ \)</span>, <span class="math notranslate nohighlight">\(ğ¸^ğ‘¡\)</span> with extracted features ğ‘‚ by aligning them with individual review <span class="math notranslate nohighlight">\(ğ¼^ğ‘ \)</span>, <span class="math notranslate nohighlight">\(ğ¼^ğ‘¡\)</span>. Finally, the regressor predicts an accurate rating that the user will give on an item. Here, shared parameters across two domains are common FE and a domain discriminator.</p>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">string</span> <span class="kn">import</span> <span class="n">punctuation</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="params">
<h3>Params<a class="headerlink" href="#params" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="loading">
<h3>Loading<a class="headerlink" href="#loading" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># !wget -q --show-progress https://anonymous.4open.science/api/repo/DaRE-9CC9/file/DaRE/Musical_Instruments.json
# !wget -q --show-progress https://anonymous.4open.science/api/repo/DaRE-9CC9/file/DaRE/Patio_Lawn_and_Garden.json

!wget -q --show-progress https://github.com/sparsh-ai/coldstart-recsys/raw/main/data/DaRE/Musical_Instruments.zip
!unzip Musical_Instruments.zip

!wget -q --show-progress https://github.com/sparsh-ai/coldstart-recsys/raw/main/data/DaRE/Patio_Lawn_and_Garden.zip
!unzip Patio_Lawn_and_Garden.zip
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!head -1 Musical_Instruments.json
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&quot;reviewerID&quot;: &quot;A2IBPI20UZIR0U&quot;, &quot;asin&quot;: &quot;1384719342&quot;, &quot;reviewerName&quot;: &quot;cassandra tu \&quot;Yeah, well, that&#39;s just like, u...&quot;, &quot;helpful&quot;: [0, 0], &quot;reviewText&quot;: &quot;Not much to write about here, but it does exactly what it&#39;s supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,&quot;, &quot;overall&quot;: 5.0, &quot;summary&quot;: &quot;good&quot;, &quot;unixReviewTime&quot;: 1393545600, &quot;reviewTime&quot;: &quot;02 28, 2014&quot;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!wget -q --show-progress https://github.com/allenai/spv2/raw/master/model/glove.6B.100d.txt.gz
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>glove.6B.100d.txt.g 100%[===================&gt;] 128.18M   148MB/s    in 0.9s    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!gunzip glove.6B.100d.txt.gz
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">Â¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">read_dataset</span><span class="p">(</span><span class="n">s_path</span><span class="p">,</span> <span class="n">t_path</span><span class="p">):</span>
    <span class="c1"># Initialization</span>
    <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(),</span> <span class="nb">dict</span><span class="p">(),</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">s_data</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_valid</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">len_t_data</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Processing Source &amp; Target Data ... </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">s_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>

    <span class="c1"># Read source data and generate user &amp; item&#39;s review dict</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span> <span class="k">break</span>

        <span class="c1"># Convert str to json format</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">,</span> <span class="n">rating</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;reviewerID&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;asin&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;reviewText&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span>

            <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="n">review</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">review</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">punctuation</span><span class="p">])</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">s_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">rating</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">s_dict</span><span class="p">:</span>
            <span class="n">s_dict</span><span class="p">[</span><span class="n">user</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s_dict</span><span class="p">[</span><span class="n">user</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">]]</span>

        <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">s_dict</span><span class="p">:</span>
            <span class="n">s_dict</span><span class="p">[</span><span class="n">item</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="p">,</span> <span class="n">review</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s_dict</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="n">user</span><span class="p">,</span> <span class="n">review</span><span class="p">]]</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="c1"># For the separation of train / valid / test data in a target domain</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">t_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">len_t_data</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span> <span class="k">break</span>

    <span class="n">len_train_data</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">len_t_data</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">len_t_data</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">len_t_data</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="c1"># Read target domain&#39;s data</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">t_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span> <span class="k">break</span>

        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">,</span> <span class="n">rating</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;reviewerID&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;asin&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;reviewText&#39;</span><span class="p">],</span> <span class="n">line</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span>

            <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="n">review</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">review</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">punctuation</span><span class="p">])</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">t_dict</span> <span class="ow">and</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">t_dict</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_valid</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">len_t_data</span><span class="p">:</span>
            <span class="n">t_valid</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">rating</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_train</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">len_train_data</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">t_train</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">rating</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">t_dict</span><span class="p">:</span>
                <span class="n">t_dict</span><span class="p">[</span><span class="n">user</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">t_dict</span><span class="p">[</span><span class="n">user</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="n">item</span><span class="p">,</span> <span class="n">review</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">t_dict</span><span class="p">:</span>
                <span class="n">t_dict</span><span class="p">[</span><span class="n">item</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">user</span><span class="p">,</span> <span class="n">review</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">t_dict</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="n">user</span><span class="p">,</span> <span class="n">review</span><span class="p">]]</span>

    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="c1"># Split valid / test data</span>
    <span class="n">t_test</span><span class="p">,</span> <span class="n">t_valid</span> <span class="o">=</span> <span class="n">t_valid</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">len_t_data</span><span class="o">/</span><span class="mi">2</span><span class="p">):</span><span class="n">len_t_data</span><span class="p">],</span> <span class="n">t_valid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">int</span><span class="p">(</span><span class="n">len_t_data</span><span class="o">/</span><span class="mi">2</span><span class="p">)]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Size of Train / Valid / Test data  : </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_valid</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_test</span><span class="p">)))</span>

    <span class="c1"># Dictionary for word embedding</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">word_vector</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">word_vector</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">word_vector_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">word_vector</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">w_embed</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">word_vector_arr</span>

    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_valid</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Define GRL for common feature extraction</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientReversalFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">lambda_</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="o">-</span><span class="n">lambda_</span> <span class="o">*</span> <span class="n">grads</span>
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-definition">
<h2>Model Definition<a class="headerlink" href="#model-definition" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DaRE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DaRE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Num of CNN filter, CNN filter size 5x100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="c1"># Word embedding dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="c1"># Loss for siamese encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">s_user_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">s_item_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_user_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_item_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filters_num</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">496</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">s_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">s_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_para</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_para</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">cnn</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_user_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_item_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">cnn</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">t_user_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_item_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">cnn</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fc</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">fc</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">fc</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fc</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">t_classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">fc</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">fc</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">ans</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="c1"># Source individual review FE</span>
        <span class="n">s_u_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_user_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">c_u_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s_u_ans_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_u_ans_fea</span> <span class="o">+</span> <span class="n">c_u_ans_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">s_i_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_item_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">c_i_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s_i_ans_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_i_ans_fea</span> <span class="o">+</span> <span class="n">c_i_ans_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">s_ans_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">s_u_ans_fea</span><span class="p">,</span> <span class="n">s_i_ans_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Label of source individual review</span>
        <span class="n">s_cls_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_classifier</span><span class="p">(</span><span class="n">s_ans_fea</span><span class="p">)</span>

        <span class="c1"># Output is [Source | Target] --&gt; Masking target output for loss calculation</span>
        <span class="n">masking</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">s_ans_out</span><span class="p">,</span> <span class="n">s_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">s_cls_out</span><span class="p">,</span> <span class="n">masking</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">masking</span><span class="p">)</span>

        <span class="c1"># Source aggregated reviews FE</span>
        <span class="n">s_u_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_user_feature_extractor</span><span class="p">(</span><span class="n">user</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s_i_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_item_feature_extractor</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">s_c_u_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">user</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s_c_i_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">s_u_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_u_fea</span> <span class="o">+</span> <span class="n">s_c_u_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">s_i_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_i_fea</span> <span class="o">+</span> <span class="n">s_c_i_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">s_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">s_u_fea</span><span class="p">,</span> <span class="n">s_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Passing through encoder for aggregated review embedding</span>
        <span class="n">s_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_encoder</span><span class="p">(</span><span class="n">s_fea</span><span class="p">)</span>

        <span class="n">s_cls_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_classifier</span><span class="p">(</span><span class="n">s_fea</span><span class="p">)</span>
        <span class="n">s_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">s_cls_out</span><span class="p">,</span> <span class="n">masking</span><span class="p">)</span>

        <span class="c1"># Distance between individual review &amp; aggregated review</span>
        <span class="n">s_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">s_ans_fea</span><span class="p">,</span> <span class="n">masking</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">s_fea</span><span class="p">,</span> <span class="n">masking</span><span class="p">))</span>

        <span class="c1"># Same for target domain</span>
        <span class="n">t_u_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_user_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">c_u_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">t_u_ans_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_u_ans_fea</span> <span class="o">+</span> <span class="n">c_u_ans_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">t_i_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_item_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">c_i_ans_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">t_i_ans_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_i_ans_fea</span> <span class="o">+</span> <span class="n">c_i_ans_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">t_ans_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">t_u_ans_fea</span><span class="p">,</span> <span class="n">t_i_ans_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">t_cls_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_classifier</span><span class="p">(</span><span class="n">t_ans_fea</span><span class="p">)</span>

        <span class="n">masking</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t_ans_out</span><span class="p">,</span> <span class="n">t_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">t_cls_out</span><span class="p">,</span> <span class="n">masking</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">masking</span><span class="p">)</span>

        <span class="c1"># Target classification loss</span>
        <span class="n">t_u_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_user_feature_extractor</span><span class="p">(</span><span class="n">user</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">t_i_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_item_feature_extractor</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">t_c_u_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">user</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">t_c_i_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">t_u_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_u_fea</span> <span class="o">+</span> <span class="n">t_c_u_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">t_i_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_i_fea</span> <span class="o">+</span> <span class="n">t_c_i_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">t_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">t_u_fea</span><span class="p">,</span> <span class="n">t_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">t_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_encoder</span><span class="p">(</span><span class="n">t_fea</span><span class="p">)</span>

        <span class="n">t_cls_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_classifier</span><span class="p">(</span><span class="n">t_fea</span><span class="p">)</span>
        <span class="n">t_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">t_cls_out</span><span class="p">,</span> <span class="n">masking</span><span class="p">)</span>

        <span class="n">t_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">t_ans_fea</span><span class="p">,</span> <span class="n">masking</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">t_fea</span><span class="p">,</span> <span class="n">masking</span><span class="p">))</span>

        <span class="c1"># Discriminator label</span>
        <span class="n">s_domain_specific</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t_domain_specific</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Common source discriminator loss</span>
        <span class="n">s_c_d_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">s_c_u_fea</span><span class="p">,</span> <span class="n">s_c_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">s_c_d_fea</span> <span class="o">=</span> <span class="n">GradientReversalFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">s_c_d_fea</span><span class="p">)</span>
        <span class="n">s_c_d_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">s_c_d_fea</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">s_c_domain_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">s_c_d_fea</span><span class="p">,</span> <span class="n">s_domain_specific</span><span class="p">)</span>

        <span class="c1"># Common target discriminator loss</span>
        <span class="n">t_c_d_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">t_c_u_fea</span><span class="p">,</span> <span class="n">t_c_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">t_c_d_fea</span> <span class="o">=</span> <span class="n">GradientReversalFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">t_c_d_fea</span><span class="p">)</span>
        <span class="n">t_c_d_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">t_c_d_fea</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="n">batch_size</span><span class="p">:</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">t_c_domain_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">t_c_d_fea</span><span class="p">,</span> <span class="n">t_domain_specific</span><span class="p">)</span>

        <span class="n">domain_common_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_c_domain_loss</span> <span class="o">+</span> <span class="n">t_c_domain_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="c1"># Source specific discriminator loss</span>
        <span class="n">s_d_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">s_u_fea</span><span class="p">,</span> <span class="n">s_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">s_d_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">s_d_fea</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="c1"># Target specific discriminator loss</span>
        <span class="n">t_d_fea</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">t_u_fea</span><span class="p">,</span> <span class="n">t_i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">t_d_fea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">t_d_fea</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="n">batch_size</span><span class="p">:</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>

        <span class="n">s_domain_specific</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">s_domain_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">s_d_fea</span><span class="p">,</span> <span class="n">s_domain_specific</span><span class="p">)</span>
        <span class="n">t_domain_specific</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t_domain_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">t_d_fea</span><span class="p">,</span> <span class="n">t_domain_specific</span><span class="p">)</span>
        <span class="n">domain_specific_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_domain_loss</span> <span class="o">+</span> <span class="n">t_domain_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="n">s_ans_out</span><span class="p">,</span> <span class="n">s_out</span><span class="p">,</span> <span class="n">s_label</span><span class="p">,</span> <span class="n">s_dist</span><span class="p">,</span> <span class="n">t_ans_out</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="n">t_label</span><span class="p">,</span> <span class="n">t_dist</span><span class="p">,</span> <span class="n">domain_common_loss</span><span class="p">,</span> <span class="n">domain_specific_loss</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="clean-strings-for-reviews">
<h2>Clean strings for reviews<a class="headerlink" href="#clean-strings-for-reviews" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">clean_str</span><span class="p">(</span><span class="n">string</span><span class="p">):</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[^A-Za-z0-9]&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot; </span><span class="se">\&#39;</span><span class="s2">s&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;ve&quot;</span><span class="p">,</span> <span class="s2">&quot; </span><span class="se">\&#39;</span><span class="s2">ve&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;n\&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot; n</span><span class="se">\&#39;</span><span class="s2">t&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;re&quot;</span><span class="p">,</span> <span class="s2">&quot; </span><span class="se">\&#39;</span><span class="s2">re&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;d&quot;</span><span class="p">,</span> <span class="s2">&quot; </span><span class="se">\&#39;</span><span class="s2">d&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;ll&quot;</span><span class="p">,</span> <span class="s2">&quot; </span><span class="se">\&#39;</span><span class="s2">ll&quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot; , &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;!&quot;</span><span class="p">,</span> <span class="s2">&quot; ! &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\(&quot;</span><span class="p">,</span> <span class="s2">&quot; \( &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\)&quot;</span><span class="p">,</span> <span class="s2">&quot; \) &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\?&quot;</span><span class="p">,</span> <span class="s2">&quot; \? &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s{2,}&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s{2,}&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;sssss &quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="review-embedding-layer">
<h2>Review embedding layer<a class="headerlink" href="#review-embedding-layer" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">valid_idx</span><span class="p">):</span>
    <span class="c1"># Return embedded vector [user, item, rev_ans, rat]</span>
    <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="mi">500</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">rat</span> <span class="o">=</span> <span class="n">s_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">s_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">s_data</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">u_rev</span><span class="p">,</span> <span class="n">i_rev</span><span class="p">,</span> <span class="n">ans_rev</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">s_dict</span><span class="p">[</span><span class="n">u</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">u_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">s_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">u</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">i_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">s_dict</span><span class="p">[</span><span class="n">u</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">ans_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">u_rev</span> <span class="o">=</span> <span class="n">u_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">u_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">i_rev</span> <span class="o">=</span> <span class="n">i_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">i_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">ans_rev</span> <span class="o">=</span> <span class="n">ans_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">ans_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="n">u_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span>
        <span class="n">i_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span>
        <span class="n">ans_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span>
        <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">rat</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">valid_idx</span><span class="p">:</span>
        <span class="n">u_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">i_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ans_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ans_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">rat</span> <span class="o">=</span> <span class="n">t_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">t_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">t_data</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">u_rev</span><span class="p">,</span> <span class="n">i_rev</span><span class="p">,</span> <span class="n">ans_rev</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">t_dict</span><span class="p">[</span><span class="n">u</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">u_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">t_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">u</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">i_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="n">reviews</span> <span class="o">=</span> <span class="n">t_dict</span><span class="p">[</span><span class="n">u</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">review</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
                <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">rev</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">clean_str</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="n">rev</span> <span class="o">=</span> <span class="n">w_embed</span><span class="p">[</span><span class="n">rev</span><span class="p">]</span>
                        <span class="n">ans_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rev</span><span class="p">)</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
                            <span class="k">break</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">continue</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">u_rev</span> <span class="o">=</span> <span class="n">u_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">u_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">i_rev</span> <span class="o">=</span> <span class="n">i_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">i_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">ans_rev</span> <span class="o">=</span> <span class="n">ans_rev</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">limit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lis</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="n">pend</span> <span class="o">=</span> <span class="n">limit</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pend</span><span class="p">):</span>
                <span class="n">ans_rev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lis</span><span class="p">)</span>

        <span class="n">u_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u_rev</span><span class="p">)</span>
        <span class="n">i_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_rev</span><span class="p">)</span>
        <span class="n">ans_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ans_rev</span><span class="p">)</span>
        <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">rat</span><span class="p">])</span>

    <span class="n">u_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">i_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ans_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ans_embed</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-function">
<h2>Training function<a class="headerlink" href="#training-function" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">learning</span><span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="c1"># Model</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Start Training ... </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">enc_loss_ratio</span><span class="p">,</span> <span class="n">domain_loss_ratio</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DaRE</span><span class="p">()</span>
    <span class="c1"># After 1 epoch, load trained parameters</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="c1"># Make batch</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">s_batch</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">t_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">batch_data</span><span class="p">,</span> <span class="n">zip_size</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">s_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s_batch</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_batch</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">source_x</span><span class="p">,</span> <span class="n">target_x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">batch_data</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">zip_size</span><span class="p">):</span>
        <span class="c1"># Pre processing</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">source_x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="n">batch_size</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="c1"># Get embedding of user and item reviews</span>
        <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">pre_processing</span><span class="p">(</span><span class="n">source_x</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">target_x</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">s_ans_out</span><span class="p">,</span> <span class="n">s_out</span><span class="p">,</span> <span class="n">s_label</span><span class="p">,</span> <span class="n">s_dist</span><span class="p">,</span> <span class="n">t_ans_out</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="n">t_label</span><span class="p">,</span> <span class="n">t_dist</span><span class="p">,</span> \
        <span class="n">c_domain_loss</span><span class="p">,</span> <span class="n">domain_loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

        <span class="c1"># Loss</span>
        <span class="n">s_ans_loss</span><span class="p">,</span> <span class="n">s_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">s_ans_out</span><span class="p">,</span> <span class="n">s_label</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="p">(</span><span class="n">s_out</span><span class="p">,</span> <span class="n">s_label</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">t_ans_loss</span><span class="p">,</span> <span class="n">t_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">t_ans_out</span><span class="p">,</span> <span class="n">t_label</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="p">(</span><span class="n">t_out</span><span class="p">,</span> <span class="n">t_label</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="c1"># Train</span>
        <span class="n">loss_func</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_loss</span> <span class="o">+</span> <span class="n">t_loss</span> <span class="o">+</span> <span class="n">s_ans_loss</span> <span class="o">+</span> <span class="n">t_ans_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> \
                    <span class="p">(</span><span class="n">s_dist</span> <span class="o">+</span> <span class="n">t_dist</span><span class="p">)</span> <span class="o">*</span> <span class="n">enc_loss_ratio</span> <span class="o">+</span> <span class="p">(</span><span class="n">c_domain_loss</span> <span class="o">+</span> <span class="n">domain_loss</span><span class="p">)</span> <span class="o">*</span> <span class="n">domain_loss_ratio</span>

        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss_func</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save</span><span class="p">)</span>
              
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Prediction Loss / Encoder Loss / Domain Loss: </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%.2f</span><span class="s1"> </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span>
              <span class="p">(</span><span class="n">s_loss</span><span class="p">,</span> <span class="n">t_loss</span><span class="p">,</span> <span class="n">s_dist</span><span class="p">,</span> <span class="n">t_dist</span><span class="p">,</span> <span class="n">c_domain_loss</span><span class="p">,</span> <span class="n">domain_loss</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="validation-inference-function">
<h2>Validation &amp; Inference function<a class="headerlink" href="#validation-inference-function" title="Permalink to this headline">Â¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">valid</span><span class="p">(</span><span class="n">v_data</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="n">write_file</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DaRE</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">t_user_feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">t_user_feature_extractor</span>
    <span class="n">t_item_feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">t_item_feature_extractor</span>
    <span class="n">t_encoder</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">t_encoder</span>
    <span class="n">t_clf</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">t_classifier</span>

    <span class="n">c_user_feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">c_user_feature_extractor</span>
    <span class="n">c_item_feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">c_item_feature_extractor</span>

    <span class="n">v_batch</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">v_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">v_loss</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">v_data</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">v_batch</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">pre_processing</span><span class="p">(</span><span class="n">v_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">v_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Target rating encoder</span>
            <span class="n">c_u_fea</span> <span class="o">=</span> <span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">c_i_fea</span> <span class="o">=</span> <span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">t_u_fea</span> <span class="o">=</span> <span class="n">t_user_feature_extractor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">t_i_fea</span> <span class="o">=</span> <span class="n">t_item_feature_extractor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">u_fea</span><span class="p">,</span> <span class="n">i_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_u_fea</span> <span class="o">+</span> <span class="n">t_u_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">c_i_fea</span> <span class="o">+</span> <span class="n">t_i_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="n">t_fea</span> <span class="o">=</span> <span class="n">t_encoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">u_fea</span><span class="p">,</span> <span class="n">i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

            <span class="n">t_out</span> <span class="o">=</span> <span class="n">t_clf</span><span class="p">(</span><span class="n">t_fea</span><span class="p">)</span>

            <span class="n">v_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">t_out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">v_loss</span> <span class="o">=</span> <span class="n">v_loss</span> <span class="o">/</span> <span class="n">idx</span>

    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">t_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">t_loss</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">t_data</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">t_batch</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">u_embed</span><span class="p">,</span> <span class="n">i_embed</span><span class="p">,</span> <span class="n">ans_embed</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">pre_processing</span><span class="p">(</span><span class="n">t_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Target rating encoder</span>
            <span class="n">c_u_fea</span> <span class="o">=</span> <span class="n">c_user_feature_extractor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">c_i_fea</span> <span class="o">=</span> <span class="n">c_item_feature_extractor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">t_u_fea</span> <span class="o">=</span> <span class="n">t_user_feature_extractor</span><span class="p">(</span><span class="n">u_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">t_i_fea</span> <span class="o">=</span> <span class="n">t_item_feature_extractor</span><span class="p">(</span><span class="n">i_embed</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">u_fea</span><span class="p">,</span> <span class="n">i_fea</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_u_fea</span> <span class="o">+</span> <span class="n">t_u_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">c_i_fea</span> <span class="o">+</span> <span class="n">t_i_fea</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="n">t_fea</span> <span class="o">=</span> <span class="n">t_encoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">u_fea</span><span class="p">,</span> <span class="n">i_fea</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

            <span class="n">t_out</span> <span class="o">=</span> <span class="n">t_clf</span><span class="p">(</span><span class="n">t_fea</span><span class="p">)</span>

            <span class="n">t_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">t_out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">t_loss</span> <span class="o">=</span> <span class="n">t_loss</span> <span class="o">/</span> <span class="n">idx</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss: </span><span class="si">%.4f</span><span class="s1"> </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">v_loss</span><span class="p">,</span> <span class="n">t_loss</span><span class="p">))</span>

    <span class="n">w</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">write_file</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="n">w</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%.6f</span><span class="s1"> </span><span class="si">%.6f</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">v_loss</span><span class="p">,</span> <span class="n">t_loss</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Define paths for source &amp; target domain</span>
    <span class="n">source_path</span> <span class="o">=</span> <span class="s1">&#39;./Musical_Instruments.json&#39;</span>
    <span class="n">target_path</span> <span class="o">=</span> <span class="s1">&#39;./Patio_Lawn_and_Garden.json&#39;</span>

    <span class="n">iteration</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">source_path</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_plus_&#39;</span> <span class="o">+</span> <span class="n">target_path</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Source &amp; Target domain: &#39;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="n">save</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span> <span class="o">+</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;.pth&#39;</span>
    <span class="n">write_file</span> <span class="o">=</span> <span class="s1">&#39;./Performance_&#39;</span> <span class="o">+</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;.txt&#39;</span>

    <span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_valid</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span> <span class="o">=</span> <span class="n">read_dataset</span><span class="p">(</span><span class="n">source_path</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iteration</span><span class="p">):</span>
        <span class="c1"># After 1 epoch of training -&gt; load trained parameter</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">learning</span><span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># First training</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">learning</span><span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_dict</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Validation and Test</span>
        <span class="n">valid</span><span class="p">(</span><span class="n">t_valid</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span> <span class="n">t_dict</span><span class="p">,</span> <span class="n">w_embed</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="n">write_file</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Source &amp; Target domain:  Musical_Instruments_plus_Patio_Lawn_and_Garden

Processing Source &amp; Target Data ... 

Size of Train / Valid / Test data  : 10618 / 1327 / 1327
Start Training ... 
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "71af347650b942369ec81944117b690d", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction Loss / Encoder Loss / Domain Loss: 26.19 19.32 0.90 0.98 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 23.41 18.45 0.88 0.96 0.71 0.70
Prediction Loss / Encoder Loss / Domain Loss: 22.63 18.95 0.89 0.95 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 25.88 19.87 0.85 0.99 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 22.18 17.75 0.85 0.94 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 23.46 19.79 0.85 0.93 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 24.17 16.33 0.83 0.93 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 22.42 17.31 0.84 0.93 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 23.93 15.30 0.82 0.91 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 23.86 18.12 0.80 0.91 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 22.18 14.11 0.81 0.89 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 21.92 15.48 0.80 0.89 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 20.06 16.45 0.81 0.89 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 22.45 17.11 0.78 0.90 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 20.94 17.29 0.77 0.87 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 20.91 16.61 0.79 0.86 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 21.18 14.18 0.79 0.87 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 19.32 14.04 0.77 0.88 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 20.00 13.79 0.76 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 19.25 14.63 0.77 0.84 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 20.34 16.40 0.75 0.84 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 21.31 15.37 0.78 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.34 14.22 0.77 0.85 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 18.06 15.67 0.78 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 19.77 14.69 0.75 0.86 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 19.45 12.66 0.75 0.84 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 20.59 13.38 0.75 0.84 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.46 12.78 0.74 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 18.28 14.84 0.76 0.85 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 17.07 13.31 0.78 0.84 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 19.17 13.59 0.77 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.55 13.56 0.77 0.84 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.99 13.02 0.79 0.87 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.46 13.56 0.79 0.87 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 17.96 13.23 0.78 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 16.65 12.41 0.78 0.85 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 18.07 12.43 0.78 0.89 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 18.07 12.09 0.79 0.86 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.30 12.74 0.79 0.88 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.66 12.91 0.77 0.88 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 16.05 12.10 0.81 0.86 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 17.10 11.36 0.84 0.90 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.91 12.35 0.82 0.89 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.89 10.59 0.80 0.91 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 18.01 11.06 0.83 0.88 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 15.35 11.33 0.83 0.87 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.54 11.90 0.83 0.91 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.28 10.60 0.83 0.88 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 16.92 9.99 0.82 0.92 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.32 9.83 0.84 0.95 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.64 10.82 0.84 0.91 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.72 10.95 0.87 0.92 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 16.45 10.12 0.87 0.93 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 15.14 10.28 0.86 0.91 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.65 12.22 0.86 0.92 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.75 10.60 0.88 0.91 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 15.21 9.65 0.86 0.94 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 16.14 9.03 0.87 0.96 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 13.92 10.14 0.89 0.95 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.52 10.68 0.88 0.94 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.40 10.07 0.89 0.95 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.26 9.76 0.88 0.95 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.70 10.39 0.91 0.98 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 13.39 9.28 0.91 0.97 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.68 9.87 0.91 0.95 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.63 8.26 0.91 0.99 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.14 9.65 0.95 0.98 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.14 9.24 0.94 1.01 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 15.22 11.08 0.92 0.98 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.95 9.93 0.94 1.00 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.20 11.14 0.96 1.00 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.19 9.12 0.95 1.01 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 14.07 10.03 0.96 1.01 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.93 8.18 0.95 1.00 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.81 8.93 0.94 1.04 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.96 9.35 0.99 1.06 0.68 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.31 9.75 0.96 1.04 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.85 9.77 0.97 1.06 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.61 8.85 0.99 1.06 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.84 8.53 0.97 1.05 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.51 8.09 0.97 1.05 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.41 7.21 1.00 1.09 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.55 8.58 1.00 1.07 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.12 8.21 0.98 1.05 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.25 9.96 0.98 1.05 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.02 7.32 0.97 1.10 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.33 8.29 1.00 1.05 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.68 8.76 0.99 1.08 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.31 9.12 1.01 1.08 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.78 8.95 1.01 1.10 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.91 8.41 1.01 1.09 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 13.76 9.18 1.03 1.16 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.69 8.34 1.02 1.11 0.69 0.70
Prediction Loss / Encoder Loss / Domain Loss: 13.44 8.99 0.99 1.12 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.39 8.73 1.02 1.11 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 13.28 7.76 1.07 1.12 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.99 6.93 1.07 1.13 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.43 7.38 1.04 1.15 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.18 9.46 1.07 1.10 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.20 8.52 1.05 1.13 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.88 6.39 1.06 1.14 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.31 7.68 1.08 1.14 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.47 8.75 1.07 1.16 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.21 8.59 1.07 1.15 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.93 9.08 1.06 1.19 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.13 7.63 1.07 1.17 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.51 9.24 1.08 1.19 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.68 7.18 1.08 1.18 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.35 8.94 1.10 1.18 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.38 8.38 1.08 1.21 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.87 7.75 1.08 1.21 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.16 6.84 1.12 1.20 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.21 7.08 1.09 1.19 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.27 8.08 1.10 1.19 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.31 7.00 1.11 1.21 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.23 8.35 1.10 1.18 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.55 6.46 1.11 1.22 0.70 0.70
Prediction Loss / Encoder Loss / Domain Loss: 11.66 7.30 1.10 1.23 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.86 8.17 1.11 1.21 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.58 5.96 1.13 1.18 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.10 7.73 1.12 1.23 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 14.18 9.23 1.10 1.21 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.19 8.29 1.13 1.22 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.69 7.63 1.14 1.23 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.60 8.57 1.13 1.23 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.07 7.60 1.13 1.24 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.73 7.31 1.16 1.21 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.91 8.51 1.13 1.24 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.46 8.23 1.14 1.22 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.07 6.78 1.12 1.25 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.73 8.29 1.15 1.24 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.34 7.24 1.17 1.22 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.37 8.07 1.14 1.26 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.09 6.89 1.12 1.23 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.16 8.47 1.18 1.25 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.48 8.46 1.15 1.27 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.47 7.77 1.17 1.25 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.66 7.79 1.20 1.22 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.29 7.02 1.17 1.28 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.37 7.11 1.17 1.29 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.16 7.55 1.18 1.26 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.37 7.61 1.18 1.29 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.58 6.96 1.18 1.27 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.63 7.14 1.16 1.30 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.39 6.91 1.19 1.27 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.76 7.83 1.17 1.27 0.68 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.52 8.41 1.22 1.32 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.61 7.55 1.19 1.30 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.28 7.66 1.22 1.26 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.36 7.39 1.18 1.28 0.68 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.15 7.98 1.23 1.26 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.15 7.28 1.16 1.28 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.39 8.09 1.16 1.30 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.96 7.63 1.17 1.28 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 13.01 6.18 1.20 1.32 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.85 6.98 1.18 1.33 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.40 6.63 1.20 1.32 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.95 7.56 1.20 1.27 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.09 7.95 1.21 1.27 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.20 7.31 1.24 1.28 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.69 7.65 1.22 1.28 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.98 6.39 1.21 1.35 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.04 6.63 1.19 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.78 5.65 1.23 1.31 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.71 8.12 1.23 1.30 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.32 7.39 1.22 1.25 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.64 7.33 1.21 1.31 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.79 6.05 1.25 1.31 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.17 7.73 1.23 1.32 0.68 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.48 5.87 1.24 1.31 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.85 6.21 1.22 1.28 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.73 8.40 1.28 1.28 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.91 6.48 1.22 1.30 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.75 7.16 1.24 1.30 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.51 8.69 1.23 1.30 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.70 7.21 1.22 1.32 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.76 7.78 1.22 1.28 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.22 6.73 1.27 1.32 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.62 7.58 1.23 1.31 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.85 7.73 1.23 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.01 7.00 1.21 1.31 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.96 7.59 1.22 1.36 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.68 6.91 1.25 1.31 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.66 7.10 1.28 1.32 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.97 8.53 1.25 1.29 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.23 6.47 1.23 1.28 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.36 6.55 1.23 1.32 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.64 5.51 1.26 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.78 7.56 1.23 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.36 7.91 1.24 1.35 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.34 7.03 1.23 1.36 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.74 6.10 1.22 1.29 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.94 6.76 1.25 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.95 6.80 1.24 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.40 6.67 1.28 1.34 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.36 6.86 1.29 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.83 8.11 1.22 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.86 6.86 1.22 1.33 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.19 6.73 1.29 1.34 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.75 6.73 1.24 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.98 5.54 1.24 1.35 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.82 5.64 1.27 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.81 6.69 1.26 1.34 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.72 7.37 1.28 1.35 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.13 6.80 1.26 1.34 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.35 7.04 1.29 1.33 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.49 5.77 1.24 1.40 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.85 6.91 1.27 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.73 6.50 1.25 1.43 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.98 6.87 1.22 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.77 5.95 1.22 1.35 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.23 6.74 1.27 1.39 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.89 7.48 1.27 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.19 1.23 1.38 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.87 6.55 1.27 1.40 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.82 6.87 1.25 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.05 5.98 1.23 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.67 6.29 1.29 1.40 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.11 6.42 1.25 1.39 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.27 7.94 1.28 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.67 6.47 1.30 1.37 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.93 6.26 1.26 1.33 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.08 6.09 1.32 1.38 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.81 7.77 1.24 1.33 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.56 6.49 1.27 1.37 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.37 7.45 1.31 1.36 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.50 7.36 1.32 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.33 6.24 1.26 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.60 5.58 1.28 1.37 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 12.14 5.59 1.28 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.84 6.83 1.27 1.40 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.99 5.66 1.23 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 12.05 6.58 1.24 1.35 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 11.88 5.98 1.26 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.20 6.33 1.28 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.12 6.80 1.30 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.81 6.14 1.31 1.37 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.07 5.55 1.32 1.39 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.49 6.92 1.27 1.38 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.27 6.42 1.28 1.34 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.51 7.41 1.28 1.39 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.24 5.71 1.27 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.85 7.26 1.22 1.34 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.83 5.88 1.30 1.40 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.92 6.57 1.27 1.36 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.15 1.30 1.35 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 11.19 6.58 1.27 1.33 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.65 6.82 1.25 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.51 5.66 1.29 1.31 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.01 6.35 1.29 1.34 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.69 6.55 1.25 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.95 6.89 1.27 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.76 1.28 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.05 6.21 1.26 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.17 5.48 1.22 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.88 5.13 1.31 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.66 6.92 1.25 1.36 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.41 6.17 1.27 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.92 6.23 1.27 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.74 5.07 1.25 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.69 6.84 1.33 1.33 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.82 5.64 1.32 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.63 1.25 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.38 7.27 1.31 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.46 6.19 1.29 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.24 6.53 1.26 1.35 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.63 7.06 1.28 1.32 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.29 6.36 1.26 1.37 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.05 6.48 1.29 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.74 5.69 1.30 1.42 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.90 5.53 1.26 1.43 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.08 5.91 1.27 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.70 5.73 1.25 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.07 6.99 1.28 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.74 5.71 1.28 1.39 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.04 5.18 1.28 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.56 6.41 1.31 1.38 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.90 5.96 1.24 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.18 5.65 1.25 1.40 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.13 6.59 1.28 1.31 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.74 6.55 1.24 1.36 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.98 5.92 1.30 1.41 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.18 4.12 1.29 1.39 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.29 5.88 1.26 1.34 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.29 5.65 1.27 1.35 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.43 6.38 1.26 1.33 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 11.15 6.08 1.26 1.39 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.50 6.08 1.28 1.35 0.69 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.84 6.83 1.28 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.16 5.50 1.27 1.37 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.46 6.68 1.25 1.38 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.52 5.54 1.24 1.36 0.67 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.64 8.08 1.25 1.32 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.94 6.50 1.25 1.35 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.80 5.36 1.25 1.31 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 9.89 5.48 1.30 1.35 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.65 6.07 1.28 1.36 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.31 6.31 1.29 1.40 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.38 1.23 1.34 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.85 5.38 1.24 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.49 5.48 1.25 1.34 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.33 6.00 1.30 1.38 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.68 6.67 1.29 1.29 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.05 5.82 1.23 1.40 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.29 5.33 1.28 1.32 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.21 4.91 1.24 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.65 6.88 1.20 1.34 0.70 0.69
Prediction Loss / Encoder Loss / Domain Loss: 10.26 6.71 1.27 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.12 6.42 1.27 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.75 7.17 1.24 1.33 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.78 6.50 1.29 1.35 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.58 5.03 1.27 1.30 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.08 5.81 1.24 1.32 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.06 4.73 1.28 1.39 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.49 4.73 1.22 1.39 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.86 5.78 1.23 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.47 5.21 1.27 1.35 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.89 6.58 1.27 1.36 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.01 6.22 1.24 1.37 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.33 1.31 1.41 0.68 0.66
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4f4509f59ac048e3b4066b5aa5e6aebc", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "278978e363ce4ad39dc9683c3a875358", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 6.0532 6.0182
Start Training ... 
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "554de49e9f334bbe9e6de81e00aa6bff", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction Loss / Encoder Loss / Domain Loss: 9.92 4.63 1.27 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.16 5.18 1.26 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.64 5.82 1.25 1.33 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.70 7.07 1.26 1.35 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.87 5.80 1.27 1.27 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 11.17 5.20 1.27 1.31 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.91 6.11 1.23 1.29 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.28 6.36 1.22 1.33 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.65 5.89 1.22 1.28 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.82 6.34 1.15 1.28 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.93 4.26 1.21 1.25 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.86 5.24 1.19 1.31 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.16 5.43 1.17 1.25 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.09 5.34 1.15 1.26 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.14 5.20 1.15 1.22 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.26 6.09 1.16 1.26 0.67 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.19 5.78 1.14 1.19 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.51 1.12 1.22 0.67 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.47 4.67 1.12 1.16 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.15 6.94 1.08 1.21 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.59 6.58 1.07 1.08 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.26 5.97 1.04 1.11 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.17 5.90 1.04 1.10 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.10 6.19 1.05 1.09 0.68 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.98 6.01 1.00 1.09 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.26 4.66 1.00 1.04 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 7.67 6.74 1.01 1.08 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.81 5.43 1.01 1.08 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.47 5.23 0.94 1.06 0.66 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.06 5.67 0.96 0.99 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.96 5.88 0.93 1.03 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.70 5.62 0.92 0.95 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 10.42 6.35 0.92 0.99 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.95 6.23 0.92 0.98 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.86 5.17 0.90 0.99 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.73 0.91 0.98 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.34 5.61 0.87 0.93 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.88 6.44 0.89 0.92 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.32 4.90 0.86 0.94 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.27 5.45 0.83 0.92 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.85 6.09 0.81 0.88 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.51 6.59 0.85 0.86 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.80 5.74 0.81 0.86 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.60 6.34 0.81 0.87 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.78 5.66 0.81 0.83 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.57 6.82 0.76 0.84 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.89 6.03 0.78 0.84 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.21 4.44 0.78 0.84 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.78 6.12 0.77 0.82 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.53 5.30 0.75 0.80 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.87 5.98 0.72 0.80 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.03 5.28 0.77 0.79 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.25 6.12 0.73 0.77 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.28 4.88 0.76 0.79 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.94 5.48 0.72 0.77 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.05 5.33 0.71 0.77 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.51 5.69 0.70 0.75 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.50 5.75 0.72 0.74 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.24 6.00 0.69 0.74 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.60 5.37 0.68 0.72 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.40 6.01 0.66 0.69 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.37 6.85 0.69 0.71 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.39 6.63 0.65 0.70 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.56 6.13 0.65 0.70 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.95 5.62 0.66 0.70 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.23 4.53 0.66 0.70 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.99 6.17 0.66 0.67 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.76 5.78 0.64 0.67 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.55 5.16 0.63 0.68 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.11 6.31 0.66 0.66 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.65 4.78 0.60 0.66 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.54 5.18 0.62 0.63 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.51 6.67 0.60 0.64 0.67 0.67
Prediction Loss / Encoder Loss / Domain Loss: 10.34 5.75 0.63 0.65 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.31 6.23 0.59 0.63 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.49 5.76 0.59 0.64 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.60 0.58 0.61 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.53 4.85 0.59 0.60 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.47 5.42 0.57 0.63 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.10 5.15 0.59 0.60 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.90 4.95 0.56 0.60 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.02 5.13 0.58 0.58 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.31 5.81 0.56 0.60 0.66 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.89 5.60 0.57 0.60 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.35 4.38 0.56 0.59 0.70 0.68
Prediction Loss / Encoder Loss / Domain Loss: 9.57 4.77 0.57 0.61 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.96 4.91 0.55 0.57 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.20 4.95 0.55 0.57 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.42 5.44 0.54 0.58 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.41 6.07 0.54 0.55 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.33 5.45 0.56 0.55 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.15 4.55 0.54 0.55 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.40 4.69 0.53 0.54 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.95 4.89 0.52 0.53 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.97 5.79 0.52 0.53 0.69 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.69 5.54 0.52 0.54 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.76 4.91 0.51 0.54 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.81 5.44 0.52 0.54 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.93 4.63 0.53 0.54 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.79 6.12 0.52 0.51 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.59 4.58 0.52 0.52 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.59 5.10 0.52 0.52 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.44 4.85 0.51 0.51 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.94 5.36 0.50 0.52 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 10.24 5.31 0.49 0.53 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.25 4.37 0.50 0.50 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.21 4.97 0.48 0.50 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.89 4.77 0.47 0.52 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.76 5.18 0.48 0.51 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.33 4.51 0.48 0.50 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.99 5.54 0.48 0.50 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.96 0.47 0.50 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.04 5.21 0.48 0.49 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.30 5.17 0.48 0.49 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.69 0.46 0.50 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.54 4.83 0.47 0.48 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.51 5.38 0.48 0.49 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.95 4.45 0.46 0.48 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.97 4.51 0.44 0.47 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.82 4.53 0.47 0.48 0.68 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.75 4.99 0.46 0.48 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.10 4.45 0.46 0.46 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.41 4.63 0.45 0.47 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.73 4.32 0.47 0.45 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.20 5.99 0.45 0.46 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.09 5.23 0.46 0.46 0.66 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.48 0.46 0.46 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.15 5.31 0.46 0.44 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.64 5.92 0.44 0.45 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.96 0.46 0.46 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.45 4.61 0.45 0.46 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.85 5.31 0.44 0.44 0.67 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.91 4.48 0.47 0.45 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.89 4.81 0.43 0.44 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.64 4.09 0.43 0.43 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.11 5.13 0.44 0.45 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.24 4.84 0.45 0.44 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.35 5.15 0.42 0.44 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.73 3.46 0.45 0.45 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 9.01 4.88 0.42 0.45 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.63 5.08 0.44 0.43 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.95 5.17 0.43 0.43 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.17 5.37 0.45 0.44 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.98 3.78 0.44 0.42 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 9.19 3.81 0.44 0.42 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.28 0.43 0.42 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.38 4.76 0.43 0.43 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.57 5.12 0.43 0.43 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 9.50 5.07 0.42 0.44 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.63 5.05 0.43 0.42 0.72 0.68
Prediction Loss / Encoder Loss / Domain Loss: 8.61 5.34 0.41 0.44 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.38 4.63 0.42 0.42 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.23 4.97 0.41 0.42 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.86 4.45 0.42 0.42 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.33 5.09 0.40 0.43 0.69 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.76 4.66 0.41 0.41 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 8.30 5.14 0.41 0.41 0.71 0.68
Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.54 0.40 0.41 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.62 5.00 0.42 0.40 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.93 4.56 0.40 0.42 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.08 4.39 0.42 0.43 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.24 4.52 0.40 0.41 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.59 5.05 0.40 0.41 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.57 4.60 0.42 0.41 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.18 5.88 0.40 0.39 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.37 5.56 0.41 0.40 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.87 3.93 0.39 0.40 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.65 3.68 0.41 0.39 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.51 4.10 0.40 0.40 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.39 3.65 0.41 0.40 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.01 4.99 0.40 0.39 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.53 4.73 0.41 0.40 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.08 4.32 0.40 0.40 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.95 5.64 0.39 0.38 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.91 0.40 0.40 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.08 4.73 0.40 0.39 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 9.09 4.53 0.40 0.39 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.96 5.19 0.39 0.40 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.33 3.36 0.40 0.40 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.79 3.07 0.40 0.39 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.36 4.01 0.40 0.39 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.99 4.62 0.39 0.40 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.54 4.86 0.38 0.39 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.52 4.56 0.40 0.39 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.26 4.67 0.39 0.38 0.71 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.97 5.09 0.41 0.39 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.95 4.42 0.39 0.38 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.82 4.27 0.41 0.39 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.12 4.85 0.39 0.39 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.82 4.67 0.40 0.39 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.56 4.25 0.40 0.38 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.23 5.13 0.39 0.38 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.96 3.71 0.41 0.38 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.94 4.53 0.39 0.39 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.36 4.21 0.39 0.39 0.71 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.89 4.56 0.39 0.38 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.90 4.19 0.38 0.39 0.70 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.98 4.32 0.39 0.38 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.15 4.53 0.40 0.39 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.16 0.39 0.39 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.57 4.21 0.39 0.39 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.71 4.73 0.39 0.40 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.33 4.52 0.37 0.38 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.32 0.38 0.38 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.12 3.94 0.37 0.38 0.67 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.87 0.38 0.37 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.00 3.55 0.38 0.38 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.33 4.51 0.37 0.37 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.58 4.90 0.38 0.37 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.70 4.97 0.38 0.39 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.59 4.57 0.37 0.37 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.13 4.37 0.37 0.38 0.68 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.68 4.46 0.39 0.37 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.89 4.78 0.37 0.38 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.28 5.49 0.39 0.36 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.30 4.99 0.38 0.37 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.63 0.37 0.37 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.24 4.80 0.37 0.37 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.23 4.06 0.38 0.37 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.53 0.38 0.37 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.70 5.19 0.37 0.37 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.56 0.38 0.38 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.33 0.36 0.35 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.01 0.38 0.39 0.67 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.47 5.28 0.39 0.37 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.18 4.18 0.38 0.38 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.32 0.37 0.39 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.35 4.06 0.37 0.38 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.07 4.45 0.38 0.38 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.29 0.36 0.39 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.03 4.62 0.38 0.37 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.78 4.18 0.36 0.37 0.71 0.67
Prediction Loss / Encoder Loss / Domain Loss: 7.71 3.93 0.37 0.39 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.08 4.75 0.39 0.38 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.26 4.25 0.38 0.38 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.64 4.26 0.37 0.36 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.77 5.54 0.37 0.37 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.42 3.65 0.37 0.36 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.46 4.96 0.39 0.35 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.56 4.18 0.38 0.36 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.95 3.58 0.37 0.37 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.91 0.37 0.36 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.01 2.85 0.37 0.36 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.32 4.74 0.38 0.36 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.07 4.15 0.36 0.35 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.43 4.14 0.38 0.36 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.41 0.37 0.37 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.00 4.87 0.36 0.36 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.09 4.07 0.38 0.38 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.23 4.27 0.35 0.37 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.82 4.05 0.36 0.35 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.36 3.87 0.37 0.37 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.96 3.70 0.36 0.36 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.55 0.35 0.35 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.12 3.82 0.37 0.34 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.49 3.31 0.36 0.35 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.71 4.67 0.35 0.37 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.35 0.35 0.35 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.76 4.53 0.36 0.36 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.21 0.37 0.37 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.62 4.00 0.37 0.35 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.58 3.48 0.37 0.36 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.35 3.81 0.37 0.35 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.84 4.13 0.37 0.35 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.20 3.45 0.36 0.35 0.72 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.32 4.16 0.36 0.36 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.38 4.28 0.36 0.36 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.96 4.48 0.36 0.36 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 8.38 4.14 0.36 0.36 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.05 4.72 0.37 0.35 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.14 4.08 0.36 0.37 0.67 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.69 3.59 0.36 0.35 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.74 4.07 0.36 0.35 0.69 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.60 3.66 0.37 0.36 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.95 3.92 0.36 0.35 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.27 3.15 0.37 0.37 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.67 4.16 0.36 0.36 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.71 0.36 0.38 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.10 4.45 0.36 0.37 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.36 3.74 0.37 0.38 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.61 4.56 0.36 0.40 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.11 0.36 0.39 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 8.09 3.72 0.36 0.40 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.85 4.21 0.36 0.40 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.04 3.89 0.37 0.44 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.18 3.38 0.36 0.42 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.40 3.63 0.36 0.42 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.86 3.37 0.35 0.46 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.03 4.49 0.36 0.44 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.23 0.35 0.46 0.67 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.97 4.23 0.36 0.45 0.67 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.19 3.66 0.36 0.45 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.00 3.72 0.36 0.45 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.55 3.28 0.35 0.46 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.17 3.89 0.36 0.45 0.71 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.29 3.64 0.34 0.46 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.51 0.35 0.46 0.67 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.78 4.18 0.34 0.45 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.70 4.23 0.34 0.45 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.79 4.11 0.35 0.44 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.04 0.36 0.44 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.91 0.35 0.44 0.70 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.67 3.29 0.36 0.44 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 8.07 3.35 0.36 0.43 0.67 0.62
Prediction Loss / Encoder Loss / Domain Loss: 7.10 3.23 0.36 0.45 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.09 3.82 0.35 0.43 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.49 3.57 0.37 0.42 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.70 4.00 0.36 0.42 0.67 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.54 3.75 0.35 0.42 0.69 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.24 3.80 0.35 0.40 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.28 0.35 0.40 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.35 3.44 0.35 0.40 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.51 4.16 0.35 0.40 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.12 3.53 0.35 0.40 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.39 3.46 0.35 0.39 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.63 4.24 0.37 0.39 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.94 3.35 0.36 0.39 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.87 4.68 0.37 0.39 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.30 0.36 0.39 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.66 4.10 0.35 0.38 0.71 0.63
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "e01a372c721b4ad987379973cba79488", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "1efeebb504b742f8b4019823650a32ef", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss: 3.9323 3.9282
Start Training ... 
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "571c9d9e2b5547d1af69e1d4f650d17c", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction Loss / Encoder Loss / Domain Loss: 6.04 2.99 0.36 0.38 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.33 3.03 0.35 0.38 0.68 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.74 3.27 0.34 0.38 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.86 3.32 0.35 0.37 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.29 4.19 0.34 0.37 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.57 4.05 0.36 0.37 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.53 3.91 0.34 0.37 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.46 3.65 0.36 0.35 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.20 3.46 0.35 0.35 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.01 3.39 0.35 0.37 0.72 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.53 3.06 0.35 0.36 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.94 3.20 0.34 0.35 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.15 3.83 0.34 0.36 0.72 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.03 4.10 0.35 0.35 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.74 4.01 0.35 0.35 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.38 3.74 0.35 0.34 0.72 0.66
Prediction Loss / Encoder Loss / Domain Loss: 5.71 3.75 0.35 0.35 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.03 3.43 0.36 0.35 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.22 3.70 0.34 0.37 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.78 3.35 0.34 0.35 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.74 3.54 0.34 0.36 0.68 0.62
Prediction Loss / Encoder Loss / Domain Loss: 7.22 3.45 0.33 0.34 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.76 3.07 0.33 0.34 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.72 3.66 0.32 0.35 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.13 4.21 0.34 0.35 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.24 3.61 0.33 0.35 0.68 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.15 3.44 0.35 0.34 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.01 4.08 0.33 0.34 0.68 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.95 3.08 0.34 0.34 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.47 4.35 0.34 0.33 0.69 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.13 2.99 0.33 0.34 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.25 3.40 0.34 0.34 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.30 3.33 0.33 0.34 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.51 3.87 0.34 0.34 0.68 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.90 3.54 0.33 0.34 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.87 3.20 0.33 0.33 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.69 0.33 0.32 0.72 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.00 3.88 0.32 0.33 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.67 3.61 0.33 0.34 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.69 3.71 0.32 0.33 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.31 3.43 0.33 0.33 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.42 3.51 0.32 0.33 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.43 3.70 0.34 0.34 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.69 3.17 0.33 0.34 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.51 4.09 0.33 0.33 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.10 3.85 0.32 0.32 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.31 3.64 0.33 0.34 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.19 3.80 0.33 0.33 0.70 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.19 3.29 0.32 0.33 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.32 0.33 0.32 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.61 4.03 0.32 0.33 0.73 0.66
Prediction Loss / Encoder Loss / Domain Loss: 7.06 4.06 0.33 0.33 0.73 0.66
Prediction Loss / Encoder Loss / Domain Loss: 6.88 3.66 0.32 0.33 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.41 3.25 0.32 0.32 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 7.04 3.22 0.32 0.32 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.38 2.94 0.32 0.32 0.68 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.46 4.51 0.32 0.32 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.37 3.09 0.31 0.32 0.70 0.65
Prediction Loss / Encoder Loss / Domain Loss: 7.21 4.29 0.31 0.33 0.69 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.44 3.43 0.33 0.31 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.97 2.93 0.32 0.32 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.44 3.85 0.33 0.31 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.83 3.64 0.33 0.32 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.57 3.17 0.31 0.31 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.56 3.45 0.31 0.32 0.68 0.61
Prediction Loss / Encoder Loss / Domain Loss: 6.28 3.23 0.32 0.32 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.10 3.82 0.31 0.31 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.21 2.82 0.32 0.31 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.08 2.97 0.31 0.31 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.21 3.62 0.32 0.32 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.39 2.86 0.32 0.31 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.76 3.96 0.32 0.32 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.55 3.02 0.31 0.31 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.53 0.31 0.31 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.06 3.43 0.31 0.31 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.54 3.30 0.31 0.30 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.57 2.82 0.31 0.32 0.71 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.08 3.33 0.32 0.30 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.24 3.86 0.31 0.31 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.32 0.32 0.30 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.43 4.00 0.31 0.31 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.86 3.62 0.30 0.31 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.63 3.75 0.31 0.31 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.67 0.32 0.31 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.11 3.29 0.31 0.30 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.87 3.80 0.31 0.30 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.37 3.75 0.32 0.30 0.71 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.20 0.31 0.31 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.60 3.43 0.30 0.30 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.76 2.93 0.31 0.30 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.77 3.57 0.31 0.31 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.08 3.47 0.32 0.31 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.49 2.85 0.31 0.29 0.75 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.91 3.33 0.32 0.31 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.29 2.99 0.31 0.31 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.15 3.72 0.30 0.31 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.86 3.59 0.31 0.30 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.16 2.78 0.31 0.30 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.13 2.83 0.30 0.31 0.70 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.52 2.66 0.31 0.30 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.28 3.06 0.30 0.31 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.78 3.01 0.31 0.30 0.72 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.63 3.63 0.30 0.30 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.33 2.93 0.31 0.29 0.67 0.61
Prediction Loss / Encoder Loss / Domain Loss: 6.00 3.27 0.31 0.29 0.71 0.61
Prediction Loss / Encoder Loss / Domain Loss: 6.13 3.05 0.31 0.30 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.03 3.73 0.30 0.29 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.92 2.82 0.30 0.30 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.12 3.22 0.30 0.30 0.73 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.80 3.09 0.31 0.30 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.12 3.41 0.30 0.29 0.73 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.00 3.08 0.30 0.30 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.67 3.14 0.30 0.29 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.93 3.33 0.30 0.30 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.16 3.23 0.30 0.29 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.86 3.54 0.31 0.29 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.33 0.29 0.29 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.35 3.40 0.29 0.30 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.68 3.60 0.30 0.30 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.43 3.44 0.30 0.29 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.26 3.01 0.30 0.29 0.70 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.96 2.71 0.30 0.29 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.75 3.58 0.30 0.29 0.70 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.98 3.32 0.30 0.29 0.71 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.46 2.98 0.30 0.29 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.84 2.86 0.31 0.29 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.91 3.03 0.31 0.29 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.77 3.09 0.31 0.30 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 6.38 3.29 0.30 0.30 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.00 2.67 0.30 0.29 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.32 3.37 0.30 0.28 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.71 3.21 0.30 0.29 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.10 0.31 0.28 0.73 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.90 3.18 0.30 0.29 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.92 3.42 0.29 0.29 0.75 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.17 2.87 0.29 0.30 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.11 3.09 0.29 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.75 3.12 0.30 0.29 0.72 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.24 2.53 0.30 0.27 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.95 3.47 0.31 0.28 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.50 2.82 0.30 0.28 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.02 0.29 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.94 0.29 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.03 2.93 0.29 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.94 3.23 0.30 0.28 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.89 3.27 0.29 0.28 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.77 3.45 0.30 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.85 0.30 0.29 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.75 3.83 0.29 0.28 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.30 3.25 0.29 0.28 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.52 3.30 0.29 0.28 0.69 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.87 0.29 0.28 0.75 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.90 2.83 0.28 0.28 0.70 0.61
Prediction Loss / Encoder Loss / Domain Loss: 4.97 3.18 0.29 0.28 0.74 0.65
Prediction Loss / Encoder Loss / Domain Loss: 6.06 3.41 0.29 0.28 0.69 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.21 2.34 0.30 0.28 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.91 3.12 0.31 0.29 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.97 0.29 0.28 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.08 2.20 0.29 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.92 3.19 0.30 0.29 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.19 2.88 0.29 0.29 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.98 3.01 0.29 0.27 0.71 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.73 2.85 0.29 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.10 2.46 0.28 0.27 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.80 2.86 0.29 0.29 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.48 2.88 0.29 0.28 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.76 2.58 0.29 0.28 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.94 3.11 0.29 0.29 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.08 2.97 0.29 0.28 0.70 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.93 3.33 0.30 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.55 2.65 0.30 0.28 0.72 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.71 0.29 0.28 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.27 2.73 0.29 0.28 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.63 2.99 0.30 0.26 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.68 0.29 0.26 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.20 2.86 0.29 0.28 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.79 3.67 0.29 0.27 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.07 2.51 0.29 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.59 0.29 0.26 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 6.05 2.91 0.28 0.27 0.78 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.87 2.66 0.28 0.27 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.28 2.69 0.29 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.86 0.29 0.28 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.78 3.09 0.28 0.29 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.52 2.35 0.28 0.27 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.76 3.11 0.29 0.27 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.78 3.26 0.28 0.28 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.47 0.28 0.26 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.39 3.13 0.29 0.26 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.58 2.81 0.29 0.27 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.62 3.06 0.29 0.27 0.70 0.61
Prediction Loss / Encoder Loss / Domain Loss: 4.83 2.52 0.28 0.27 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.62 1.85 0.28 0.27 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 6.30 2.81 0.28 0.27 0.70 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.72 2.84 0.29 0.27 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.91 0.28 0.26 0.75 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.48 2.89 0.29 0.27 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.66 2.99 0.28 0.27 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.36 2.86 0.29 0.27 0.71 0.60
Prediction Loss / Encoder Loss / Domain Loss: 5.69 3.15 0.28 0.26 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.24 2.58 0.28 0.27 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.29 0.29 0.28 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.41 2.82 0.29 0.27 0.75 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.78 0.28 0.27 0.72 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.30 2.73 0.28 0.27 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.87 2.96 0.28 0.28 0.75 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.59 0.28 0.27 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.34 0.28 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.04 2.87 0.29 0.28 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.93 0.27 0.29 0.71 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.63 3.07 0.28 0.30 0.78 0.66
Prediction Loss / Encoder Loss / Domain Loss: 5.69 2.51 0.28 0.30 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.93 0.27 0.31 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.75 2.95 0.28 0.33 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.60 0.28 0.34 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.62 0.28 0.35 0.72 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.92 2.71 0.28 0.36 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.32 3.15 0.28 0.37 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.71 2.73 0.28 0.38 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.64 0.28 0.39 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.93 2.44 0.28 0.39 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.40 0.28 0.39 0.76 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.03 2.90 0.28 0.41 0.73 0.61
Prediction Loss / Encoder Loss / Domain Loss: 5.50 2.20 0.28 0.38 0.77 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.12 2.07 0.28 0.40 0.72 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.58 2.77 0.28 0.39 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.44 2.50 0.27 0.39 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.91 2.36 0.27 0.38 0.75 0.61
Prediction Loss / Encoder Loss / Domain Loss: 4.94 2.80 0.27 0.38 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.96 2.79 0.29 0.38 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.87 0.27 0.38 0.77 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.11 2.26 0.27 0.37 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.53 2.80 0.27 0.35 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.19 2.67 0.28 0.34 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.22 3.01 0.28 0.34 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.18 2.75 0.28 0.34 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.31 2.47 0.28 0.33 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.04 3.15 0.27 0.32 0.78 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.45 3.09 0.27 0.32 0.75 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.85 2.92 0.27 0.32 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.61 2.22 0.28 0.31 0.79 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.10 2.25 0.27 0.32 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.41 1.97 0.27 0.31 0.79 0.66
Prediction Loss / Encoder Loss / Domain Loss: 5.46 2.43 0.27 0.30 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.44 2.06 0.28 0.30 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.18 2.43 0.27 0.29 0.76 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.66 2.17 0.28 0.28 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.44 2.14 0.27 0.29 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.63 0.27 0.30 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.76 2.53 0.27 0.28 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.36 2.72 0.27 0.28 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.62 3.08 0.27 0.28 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.13 2.74 0.28 0.28 0.76 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.65 2.19 0.28 0.28 0.76 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.84 2.37 0.27 0.28 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.52 2.94 0.27 0.28 0.77 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.78 2.41 0.27 0.29 0.78 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.58 2.22 0.27 0.29 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.10 2.38 0.26 0.28 0.73 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.45 2.16 0.27 0.29 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.67 2.51 0.27 0.29 0.78 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.39 0.27 0.27 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.75 2.42 0.26 0.29 0.78 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.55 2.81 0.27 0.28 0.77 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.54 2.99 0.27 0.29 0.75 0.61
Prediction Loss / Encoder Loss / Domain Loss: 4.85 2.80 0.27 0.28 0.72 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.64 0.26 0.29 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.74 0.26 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.22 2.34 0.26 0.27 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.77 2.63 0.26 0.28 0.74 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.63 2.52 0.26 0.29 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.44 2.63 0.27 0.28 0.74 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.30 2.97 0.27 0.28 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.20 0.27 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.21 2.31 0.26 0.28 0.79 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.29 0.26 0.28 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.72 2.48 0.26 0.29 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.72 1.89 0.26 0.27 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.46 2.33 0.26 0.27 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.32 1.96 0.26 0.27 0.75 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.32 2.33 0.26 0.26 0.77 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.24 0.25 0.27 0.76 0.65
Prediction Loss / Encoder Loss / Domain Loss: 5.13 2.23 0.26 0.26 0.79 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.17 1.79 0.26 0.27 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.43 0.27 0.26 0.74 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.36 2.65 0.27 0.26 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.11 2.68 0.26 0.27 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.54 2.59 0.26 0.25 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 5.41 2.44 0.26 0.26 0.77 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.15 2.22 0.26 0.26 0.78 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.20 2.21 0.26 0.27 0.76 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.46 0.26 0.27 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.01 2.83 0.26 0.25 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.29 0.26 0.25 0.74 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.56 2.68 0.26 0.26 0.77 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.74 2.47 0.26 0.26 0.77 0.66
Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.53 0.26 0.26 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.85 1.84 0.25 0.26 0.79 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.66 2.24 0.26 0.26 0.76 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.78 2.35 0.26 0.25 0.76 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.71 2.46 0.26 0.25 0.78 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.58 2.22 0.26 0.25 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.83 2.60 0.26 0.26 0.75 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.51 2.07 0.26 0.24 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.00 2.49 0.25 0.25 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.06 0.25 0.26 0.78 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.42 3.01 0.26 0.25 0.73 0.62
Prediction Loss / Encoder Loss / Domain Loss: 5.59 2.08 0.26 0.26 0.75 0.62
Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.16 0.26 0.25 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.09 2.40 0.26 0.25 0.80 0.66
Prediction Loss / Encoder Loss / Domain Loss: 5.33 2.28 0.26 0.26 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.27 2.53 0.26 0.25 0.77 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.95 2.63 0.26 0.25 0.77 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.14 2.37 0.26 0.25 0.77 0.63
Prediction Loss / Encoder Loss / Domain Loss: 4.93 2.31 0.26 0.26 0.79 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.86 1.72 0.26 0.26 0.79 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.49 2.11 0.25 0.25 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 4.42 2.07 0.26 0.26 0.76 0.65
Prediction Loss / Encoder Loss / Domain Loss: 4.30 2.33 0.27 0.26 0.76 0.64
Prediction Loss / Encoder Loss / Domain Loss: 5.02 2.37 0.26 0.26 0.77 0.65
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "f54b9b78c1cd49afa93c27ee9f5dc91b", "version_major": 2, "version_minor": 0}
</script></div>
</div>
</div>
<div class="section" id="extra-notes">
<h2>Extra Notes<a class="headerlink" href="#extra-notes" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="inference-process">
<h3>Inference Process<a class="headerlink" href="#inference-process" title="Permalink to this headline">Â¶</a></h3>
<p><center><img src='_images/T519611_3.png'></center></p></div>
<div class="section" id="domain-aware-feature-extraction-example">
<h3>Domain-Aware Feature Extraction Example<a class="headerlink" href="#domain-aware-feature-extraction-example" title="Permalink to this headline">Â¶</a></h3>
<p>Following is the example of domain-aware feature extraction from a real-world benchmark dataset Amazon.</p>
<p><center><img src='_images/T519611_4.png'></center></p>
<p><center><img src='_images/T519611_5.png'></center></p><p>We assume two phases: training and inference, with two different domains: Musical Instruments and Toys &amp; Games for cross-domain recommendation scenario. The scenario assumes a training phase with source (upper) and target (lower) domain. The difference is that a common FE (red-box) is shared across domains, while the source and target FEs (green and blue boxes) are domain-specific networks. <strong>The objective is predicting a rating that a user ğ´ gives on item 2</strong>. Excluding individual review, user ğ´â€²ğ‘  review on item 2, the source and common extractors distillate latent of user and item respectively. Specifically, for user ğ´ in ğ‘€ğ‘¢ğ‘ ğ‘–ğ‘ğ‘ğ‘™ ğ¼ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ , a source FE captures domain-specific knowledge that she makes much of sound quality, while common FE extracts domain-common information like beautiful, and nice price. The analysis for item 2 follows the same mechanism. To summarize, DaRE model not only considers domain-shareable knowledge with common FE but also reflects domain-specific information through the source and target FE.</p>
</div>
<div class="section" id="review-encoder-example">
<h3>Review Encoder Example<a class="headerlink" href="#review-encoder-example" title="Permalink to this headline">Â¶</a></h3>
<p><center><img src='_images/T519611_6.png'></center></p>
<p><center><img src='_images/T519611_7.png'></center></p><p>For the training of a review encoder, we utilize individual review that user ğ´ has written on item 2 (blue box) as another label. Taking the above figure as an example, the review encoder (purple box) takes four types of inputs which are extracted from the source and common FEs. Then, the encoder generates a single output, which contains mixed information of user ğ´ and item 2. Here, the encoder is trained to infer an individual review, negative feedback of user ğ´ who takes sound quality into account. Likewise, another encoder in a target domain can be trained in a same manner. With user and itemâ€™s previous reviews, the encoder assumes a real feedback that user will leave after purchasing an item.</p>
<p><strong>END</strong></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/coldstart-recsys",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T290734_AGGN_Cold_start_Recommendation_on_ML_100k.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">AGGN Cold-start Recommendation on ML-100k</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>